{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_2j_2DsSLptL"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.nn.parallel.distributed import DistributedDataParallel"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Distributed Training ###\n",
        "- For CNN that train in big model"
      ],
      "metadata": {
        "id": "MG6TVQQrM4BZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger(__name__)\n",
        "rank = 0\n",
        "world_size = 1"
      ],
      "metadata": {
        "id": "hfvLl1pYMRTk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 以下是用CUDA系统"
      ],
      "metadata": {
        "id": "hpQrJD07QX6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init(args):\n",
        "  global rank, world_size\n",
        "  if args.ddp:\n",
        "    assert args.rank is not None and args.world_size is not None\n",
        "    rank = args.rank\n",
        "    world_size = args.world_size\n",
        "  if world_size == 1:\n",
        "    return\n",
        "  torch.cuda.set_device(rank)\n",
        "  torch.distributed.init_process_group(\n",
        "      backend=args.ddp_backend,\n",
        "      init_method='file://' + os.path.abspath(args.rendezvous_file),\n",
        "      world_size=world_size,\n",
        "      rank=rank)\n",
        "  logger.debug(\"Distributed rensezvous went well, rank %d/%d\", rank, world_size)\n",
        "\n",
        "def average(metrics, count=1.):\n",
        "  if world_size == 1:\n",
        "    return metrics\n",
        "  tensor = torch.tensor(list(metrics) + [1], device='cuda', dtype=torch.float32)\n",
        "  tensor *= count\n",
        "  torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
        "  return (tensor[:-1] / tensor[-1]).cpu().numpy().tolist()\n",
        "\n",
        "def wrap(model):\n",
        "  if world_size == 1:\n",
        "    return model\n",
        "  else:\n",
        "    return DistributedDataParallel(\n",
        "        model,\n",
        "        device_ids=[torch.cuda.current_device()],\n",
        "        output_device=torch.cuda.current_device())\n",
        "\n",
        "def barrier():\n",
        "  if world_size > 1:\n",
        "    torch.distributed.barrier()\n",
        "\n",
        "def loader(dataset, *args, shuffle=False, klass=DataLoader, **kwargs):\n",
        "  if world_size == 1:\n",
        "    return klass(dataset, *args, shuffle=shuffle, **kwargs)\n",
        "\n",
        "  if shuffle:\n",
        "    sample = DistributedSampler(dataset)\n",
        "    return klass(dataset, *args, **kwargs, sampler=sampler)\n",
        "  else:\n",
        "    dataset = Subset(dataset, list(range(rank, len(dataset), world_size)))\n",
        "    return klass(dataset, *args, shuffle=shuffle)"
      ],
      "metadata": {
        "id": "id65jPAPMYd4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 以下是AMD ROCm, need with PyTorch 1.4x - 1.7x"
      ],
      "metadata": {
        "id": "SuHLT-XIRaIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataloader import Sampler\n",
        "def init(args):\n",
        "  global rank, world_size\n",
        "  if args.ddp:\n",
        "    assert args.rank is not None and args.world_size is not None\n",
        "    rank = args.rank\n",
        "    world_size = args.world_size\n",
        "  if world_size == 1:\n",
        "    return\n",
        "\n",
        "  torch.device('rocm:0')\n",
        "\n",
        "  torch.cuda.set_device(rank)\n",
        "  torch.distributed.init_process_group(\n",
        "      backend=args.ddp_backend,\n",
        "      init_method='file://' + os.path.abspath(args.rendezvous_file),\n",
        "      world_size=world_size,\n",
        "      rank=rank)\n",
        "  logger.debug(\"Distributed rensezvous went well, rank %d/%d\", rank, world_size)\n",
        "\n",
        "def average(metrics, count=1.):\n",
        "  if world_size == 1:\n",
        "    return metrics\n",
        "  tensor = torch.tensor(list(metrics) + [1], device='rocm:0', dtype=torch.float32)\n",
        "  tensor *= count\n",
        "  torch.distributed.all_reduce(tensor, op=torch.distributed.ReduceOp.SUM)\n",
        "  return (tensor[:-1] / tensor[-1]).cpu().numpy().tolist()\n",
        "\n",
        "def wrap(model):\n",
        "  if world_size == 1:\n",
        "    return model\n",
        "  else:\n",
        "    return DistributedDataParallel(\n",
        "        model,\n",
        "        device_ids=[torch.device('rocm:0')],\n",
        "        output_device=torch.device('rocm:0'))\n",
        "\n",
        "def barrier():\n",
        "  if world_size > 1:\n",
        "    torch.distributed.barrier()\n",
        "\n",
        "def loader(dataset, *args, shuffle=False, klass=DataLoader, **kwargs):\n",
        "  if world_size == 1:\n",
        "    return klass(dataset, *args, shuffle=shuffle, **kwargs)\n",
        "\n",
        "  if shuffle:\n",
        "    sample = DistributedSampler(dataset)\n",
        "    return klass(dataset, *args, **kwargs, sampler=sampler) #WHY there is a 报错\n",
        "  else:\n",
        "    dataset = Subset(dataset, list(range(rank, len(dataset), world_size)))\n",
        "    return klass(dataset, *args, shuffle=shuffle)"
      ],
      "metadata": {
        "id": "dbcmFtVCRZVx"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}